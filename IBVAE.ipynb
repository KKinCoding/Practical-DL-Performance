{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pyportfolioopt\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "#Helper function to generate input data of appropriate shape\n",
    "def generate_time_series_data(data, batch_size):\n",
    "  # Generate batches of data\n",
    "  num_samples = data.shape[0]\n",
    "  num_batches = num_samples // batch_size\n",
    "  batchSet = []\n",
    "  for i in range(num_batches):\n",
    "    batchSet.append(data[i * batch_size:(i + 1) * batch_size])\n",
    "  return np.asarray(batchSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and create portfolios\n",
    "Portfolio_tickers = ['AVGO', 'COST', 'FDS', 'FTNT', 'ORLY', 'REGN', 'TMO', 'TSLA', 'UNH']\n",
    "Long_tickers = ['CPB', 'K']\n",
    "Portfolio = yf.download(Portfolio_tickers, start='2021-05-03', end='2022-07-12')['Adj Close']\n",
    "Long = yf.download(Long_tickers, start='2021-05-03', end='2022-07-12')['Adj Close']\n",
    "CPB = Long[['CPB']]\n",
    "K = Long[['K']]\n",
    "Portfolio1 = pd.concat([Portfolio, CPB], axis=1)\n",
    "Portfolio2 = pd.concat([Portfolio, K], axis=1)\n",
    "Portfolio3 = pd.concat([Portfolio, Long], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "\n",
    "#Data preprocessing\n",
    "training_data_len = math.ceil(len(Portfolio3)* 0.8)\n",
    "temp = []\n",
    "tickers = Portfolio3.columns.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(Portfolio3)\n",
    "data = scaled_data.reshape(-1,11)\n",
    "print(f\"data shape: {data.shape}\")\n",
    "\n",
    "#Training data\n",
    "train_data = scaled_data[0: training_data_len, :]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(batch_size, len(train_data)):\n",
    "    x_train.append(train_data[i-batch_size:i, :])\n",
    "    y_train.append(train_data[i, :])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "print(f\"train_data shape: {train_data.shape}\")\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "#Test data\n",
    "test_data = scaled_data[training_data_len-batch_size: , : ]\n",
    "y_test = data[training_data_len:]\n",
    "x_test = []\n",
    "\n",
    "for i in range(batch_size, len(test_data)):\n",
    "  x_test.append(test_data[i-batch_size:i, :])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))\n",
    "\n",
    "print(f\"test data shape: {test_data.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the hyperparameters\n",
    "latent_dim = 32\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 50\n",
    "beta = 1 #information bottleneck coefficient\n",
    "n_stocks = np.shape(x_train)[2]\n",
    "\n",
    "#Split the data into training and test sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dataStocks, dataSPY, test_size=0.2)\n",
    "# x_train = generate_time_series_data(x_train, batch_size)\n",
    "# x_test = generate_time_series_data(x_test, batch_size)\n",
    "# y_train = generate_time_series_data(y_train, batch_size)\n",
    "# y_test = generate_time_series_data(y_test, batch_size)\n",
    "# print(f\"shapes 1: {np.shape(x_train)} and {np.shape(y_train)}\")\n",
    "\n",
    "#Reshape the data to be 3D [samples, timesteps, features]\n",
    "# x_train = x_train.reshape((-1, 1, 1))\n",
    "# x_test = x_test.reshape(-1, 1, 1)\n",
    "\n",
    "#Build the model\n",
    "inputs = tf.keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "lstm_encoder = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = lstm_encoder(inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "z_mean = tf.keras.layers.Dense(latent_dim)(encoder_outputs)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim)(encoder_outputs)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5*z_log_var)*epsilon\n",
    "\n",
    "z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "lstm_decoder = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = lstm_decoder(z, initial_state=encoder_states)\n",
    "decoder_outputs=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_stocks))(decoder_outputs)\n",
    "\n",
    "#Define the losses for IB and no-IB\n",
    "def IBLoss(inputs, decoder_outputs):\n",
    "  reconstruction_loss = tf.keras.losses.MeanSquaredError()(inputs, decoder_outputs)\n",
    "  kl_loss = -0.5*tf.reduce_mean(z_log_var - tf.square(z_mean)-tf.exp(z_log_var) + 1)\n",
    "  information_bottleneck_loss = beta*kl_loss\n",
    "  loss = reconstruction_loss + information_bottleneck_loss\n",
    "  return loss\n",
    "\n",
    "def MSELoss(inputs, decoder_outputs):\n",
    "  reconstruction_loss = tf.keras.losses.MeanSquaredError()(inputs, decoder_outputs)\n",
    "  return reconstruction_loss\n",
    "\n",
    "#Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#Compile the model\n",
    "model = tf.keras.Model(inputs, decoder_outputs)\n",
    "model.compile(optimizer=optimizer, loss=IBLoss)\n",
    "\n",
    "model2 = tf.keras.Model(inputs, decoder_outputs)\n",
    "model2.compile(optimizer=optimizer, loss=MSELoss)\n",
    "\n",
    "#Train the model\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=num_epochs)\n",
    "model2.fit(x_train, y_train, batch_size=1, epochs=num_epochs)\n",
    "\n",
    "#Evaluate the model\n",
    "predictions = np.mean(model.predict(x_test), axis=0)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "\n",
    "predictions2 = np.mean(model2.predict(x_test), axis=0)\n",
    "predictions2 = scaler.inverse_transform(predictions2)\n",
    "rmse2 = np.sqrt(np.mean(predictions2 - y_test)**2)\n",
    "\n",
    "print(f\"IB rmse: {rmse}\")\n",
    "print(f\"no IB rmse: {rmse2}\")\n",
    "\n",
    "# test_loss = model.evaluate(x_test, y_test)\n",
    "# print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSharpe(port):\n",
    "  mu = expected_returns.capm_return(port)\n",
    "  Sigma = risk_models.CovarianceShrinkage(port).ledoit_wolf()\n",
    "\n",
    "  ef = EfficientFrontier(mu, Sigma)\n",
    "  ef.max_sharpe()\n",
    "  weights = ef.clean_weights()\n",
    "\n",
    "  portfolio_mean = 0\n",
    "  portfolio_var = 0\n",
    "\n",
    "  for ticker in weights.keys():\n",
    "      portfolio_mean += weights[ticker]*mu[ticker]\n",
    "\n",
    "  for ticker1 in weights.keys():\n",
    "      for ticker2 in weights.keys():\n",
    "          portfolio_var += weights[ticker1]*weights[ticker2]*sigma[ticker1][ticker2]\n",
    "\n",
    "  portfolio_std = portfolio_var ** (1/2)\n",
    "\n",
    "  portfolio_sharpe = portfolio_mean/portfolio_std\n",
    "  return portfolio_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sharpe Ratios\\nPortfolio 1: {calcSharpe(Port1)}\\nPortfolio 2: {calcSharpe(Port2)}\\nPortfolio 3: {calcSharpe(Port3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d99d9eb2277144527de8d4eac28345056b6e76b04b7dd1ea1ada924da721680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
